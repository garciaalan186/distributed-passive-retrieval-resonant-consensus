# Core dependencies
chromadb==0.4.22
pydantic==2.5.3
numpy==1.26.3
tiktoken==0.5.2  # Token counting for tempo-normalized sharding

# ML/Embeddings - GPU-enabled PyTorch for SLM inference acceleration
# Note: CPU-only torch is ~200MB, GPU-enabled is ~2GB but provides 10-100x speedup
torch==2.1.2
sentence-transformers==2.3.1

# SLM (Small Language Model) for L2 verification
# transformers>=4.37.0 required for Qwen2 model support
transformers==4.46.0
accelerate>=0.26.0
bitsandbytes>=0.41.0  # 4-bit quantization for memory-efficient inference

# Pin huggingface_hub to ensure compatibility with transformers
huggingface_hub>=0.20.0,<0.27.0

# API
fastapi==0.109.0
uvicorn==0.27.0
python-json-logger==2.0.7
requests==2.31.0
aiohttp==3.9.1
